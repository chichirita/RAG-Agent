import os
from loguru import logger
from langchain_community.vectorstores import FAISS
from langchain_core.messages import HumanMessage, AIMessage

logger.add("log/02_Simple_RAG_PDF.log", format="{time} {level} {message}", level="DEBUG", rotation="100 KB", compression="zip")


def get_index_db():
    logger.debug('...get_index_db')
    logger.debug('Embeddings')
    from langchain_huggingface import HuggingFaceEmbeddings
    model_id = 'intfloat/multilingual-e5-large'
    model_kwargs = {'device': 'cpu'}
    # model_kwargs = {'device': 'cuda'}
    embeddings = HuggingFaceEmbeddings(
        model_name=model_id,
        model_kwargs=model_kwargs
    )

    db_file_name = 'db/db_01'
    logger.debug('–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–∞–∑—ã-–ó–Ω–∞–Ω–∏–π –∏–∑ —Ñ–∞–π–ª–∞')
    file_path = db_file_name + "/index.faiss"
    import os.path
    if os.path.exists(file_path):
        logger.debug('–£–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–∞–∑–∞-–∑–Ω–∞–Ω–∏–π')
        db = FAISS.load_local(db_file_name, embeddings, allow_dangerous_deserialization=True)

    else:
        logger.debug('–ï—â–µ –Ω–µ —Å–æ–∑–¥–∞–Ω–∞ –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–∞–∑–∞-–ó–Ω–∞–Ω–∏–π')
        from langchain_community.document_loaders import PyPDFLoader

        dir = 'pdf'
        logger.debug(f'Document loaders. dir={dir}')
        documents = []
        for root, dirs, files in os.walk(dir):
            for file in files:
                if file.endswith(".pdf"):
                    logger.debug(f'root={root} file={file}')
                    loader = PyPDFLoader(os.path.join(root, file))
                    documents.extend(loader.load())


        logger.debug('–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ chunks')
        from langchain.text_splitter import RecursiveCharacterTextSplitter

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=150)
        source_chunks = text_splitter.split_documents(documents)
        logger.debug(type(source_chunks))
        logger.debug(len(source_chunks))
        logger.debug(source_chunks[10].metadata)
        logger.debug(source_chunks[10].page_content)


        logger.debug('–í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–∞–∑–∞-–ó–Ω–∞–Ω–∏–π')
        db = FAISS.from_documents(source_chunks, embeddings)

        logger.debug('–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–∞–∑—ã-–ó–Ω–∞–Ω–∏–π –≤ —Ñ–∞–π–ª')
        db.save_local(db_file_name)

    return db

def get_message_content(topic, db, NUMBER_RELEVANT_CHUNKS):
    import re
    logger.debug('...get_message_content: Similarity search')
    docs = db.similarity_search(topic, k = NUMBER_RELEVANT_CHUNKS)
    message_content = re.sub(r'\n{2}', ' ', '\n '.join([f'\n#### {i+1} Relevant chunk ####\n' + str(doc.metadata) + '\n' + doc.page_content + '\n' for i, doc in enumerate(docs)]))
    logger.debug(message_content)
    return message_content


def get_model_response(topic, message_content):
    logger.debug('...get_model_response')

    from langchain_ollama import ChatOllama
    logger.debug('LLM')
    local_llm = "bambucha/saiga-llama3:8b"
    llm = ChatOllama(model=local_llm, temperature=0)


    rag_prompt = """–¢—ã —è–≤–ª—è–µ—à—å—Å—è –º–µ–Ω–µ–¥–∂–µ—Ä–æ–º –∫–æ–º–ø–∞–Ω–∏–∏ PolarAgency –∏ –æ—Ç–≤–µ—á–∞–µ—à—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.
    –í–æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –æ—Ç–≤–µ—Ç–∞:
    {context}
    –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
    –¢–µ–ø–µ—Ä—å –æ–∑–Ω–∞–∫–æ–º—å—Å—è —Å –≤–æ–ø—Ä–æ—Å–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
    {question}
    –î–∞–π –æ—Ç–≤–µ—Ç –æ—Ç –ª–∏—Ü–∞ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∫–æ–º–ø–∞–Ω–∏–∏ PolarAgency, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –≤—ã—à–µ—É–∫–∞–∑–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
–§   –æ—Ä–º—É–ª–∏—Ä—É–π –∫—Ä–∞—Ç–∫–æ, –Ω–µ –±–æ–ª–µ–µ —Ç—Ä—ë—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –ù–∞ —Ç—Ä–µ—Ç—å–µ —Ç–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ø—Ä–æ—Å–∏ –∫–æ–Ω—Ç–∞–∫—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è(–Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞, –ø–æ—á—Ç–∞) —Å —Ü–µ–ª—å—é –¥–∞–ª—å–Ω–µ–π—â–µ–π —Å–≤—è–∑–∏."""

    rag_prompt_formatted = rag_prompt.format(context=message_content, question=topic)
    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])
    model_response = generation.content
    logger.debug(model_response)
    return model_response

def chat_loop(db, NUMBER_RELEVANT_CHUNKS=3):
    history = []
    print("üí¨ –ß–∞—Ç —Å PolarAgency. –í–≤–µ–¥–∏—Ç–µ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞.\n")

    while True:
        user_input = input("–í—ã: ")
        if user_input.lower() in ["exit", "quit", "–≤—ã—Ö–æ–¥"]:
            print("–ß–∞—Ç –∑–∞–≤–µ—Ä—à—ë–Ω.")
            break

        message_content = get_message_content(user_input, db, NUMBER_RELEVANT_CHUNKS)

        user_questions_count = sum(isinstance(m, HumanMessage) for m in history) + 1

        context_with_history = "\n".join([
            f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {m.content}" if isinstance(m, HumanMessage) else f"PolarAgency: {m.content}"
            for m in history
        ])

        rag_prompt = f"""
–¢—ã –º–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–º–ø–∞–Ω–∏–∏ PolarAgency –∏ –æ—Ç–≤–µ—á–∞–µ—à—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{message_content}

–ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞:
{context_with_history}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
{user_input}
–î–∞–π –æ—Ç–≤–µ—Ç –æ—Ç –ª–∏—Ü–∞ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∫–æ–º–ø–∞–Ω–∏–∏ PolarAgency, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –≤—ã—à–µ—É–∫–∞–∑–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
–§–æ—Ä–º—É–ª–∏—Ä—É–π –∫—Ä–∞—Ç–∫–æ, –Ω–µ –±–æ–ª–µ–µ —Ç—Ä—ë—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –ù–∞ —Ç—Ä–µ—Ç—å–µ —Ç–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ø—Ä–æ—Å–∏ –∫–æ–Ω—Ç–∞–∫—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è(–Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞, –ø–æ—á—Ç–∞) —Å —Ü–µ–ª—å—é –¥–∞–ª—å–Ω–µ–π—â–µ–π —Å–≤—è–∑–∏.
.
"""

        if user_questions_count == 3:
            rag_prompt += """
–¢–∞–∫ –∫–∞–∫ —ç—Ç–æ —É–∂–µ —Ç—Ä–µ—Ç–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ –ø–æ–ø—Ä–æ—Å–∏ –æ—Å—Ç–∞–≤–∏—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ç–µ–ª–µ—Ñ–æ–Ω –∏–ª–∏ e-mail) –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π —Å–≤—è–∑–∏.
"""

        from langchain_ollama import ChatOllama
        llm = ChatOllama(model="bambucha/saiga-llama3:8b", temperature=0)

        print("PolarAgency: ", end="", flush=True)
        response_text = ""
        for chunk in llm.stream([HumanMessage(content=rag_prompt)]):
            token = chunk.content
            print(token, end="", flush=True)
            response_text += token
        print("\n")

        history.append(HumanMessage(content=user_input))
        history.append(AIMessage(content=response_text))


if __name__ == "__main__":
    db = get_index_db()
    chat_loop(db)
